{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alan Donahue Ch 9 Exercise\n",
    "\n",
    "http://thinkstats2.com\n",
    "\n",
    "Copyright 2016 Allen B. Downey\n",
    "\n",
    "MIT License: https://opensource.org/licenses/MIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import thinkstats2\n",
    "import thinkplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypothesisTest(object):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.MakeModel()\n",
    "        self.actual = self.TestStatistic(data)\n",
    "\n",
    "    def PValue(self, iters=1000):\n",
    "        self.test_stats = [self.TestStatistic(self.RunModel()) \n",
    "                           for _ in range(iters)]\n",
    "\n",
    "        count = sum(1 for x in self.test_stats if x >= self.actual)\n",
    "        return count / iters\n",
    "\n",
    "    def TestStatistic(self, data):\n",
    "        raise UnimplementedMethodException()\n",
    "\n",
    "    def MakeModel(self):\n",
    "        pass\n",
    "\n",
    "    def RunModel(self):\n",
    "        raise UnimplementedMethodException()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffMeansPermute(thinkstats2.HypothesisTest):\n",
    "\n",
    "    def TestStatistic(self, data):\n",
    "        group1, group2 = data\n",
    "        test_stat = abs(group1.mean() - group2.mean())\n",
    "        return test_stat\n",
    "\n",
    "    def MakeModel(self):\n",
    "        group1, group2 = self.data\n",
    "        self.n, self.m = len(group1), len(group2)\n",
    "        self.pool = np.hstack((group1, group2))\n",
    "\n",
    "    def RunModel(self):\n",
    "        np.random.shuffle(self.pool)\n",
    "        data = self.pool[:self.n], self.pool[self.n:]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import first\n",
    "\n",
    "live, firsts, others = first.MakeFrames()\n",
    "data = firsts.prglngth.values, others.prglngth.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffStdPermute(DiffMeansPermute):\n",
    "\n",
    "    def TestStatistic(self, data):\n",
    "        group1, group2 = data\n",
    "        test_stat = group1.std() - group2.std()\n",
    "        return test_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrelationPermute(thinkstats2.HypothesisTest):\n",
    "\n",
    "    def TestStatistic(self, data):\n",
    "        xs, ys = data\n",
    "        test_stat = abs(thinkstats2.Corr(xs, ys))\n",
    "        return test_stat\n",
    "\n",
    "    def RunModel(self):\n",
    "        xs, ys = self.data\n",
    "        xs = np.random.permutation(xs)\n",
    "        return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PregLengthTest(thinkstats2.HypothesisTest):\n",
    "\n",
    "    def MakeModel(self):\n",
    "        firsts, others = self.data\n",
    "        self.n = len(firsts)\n",
    "        self.pool = np.hstack((firsts, others))\n",
    "\n",
    "        pmf = thinkstats2.Pmf(self.pool)\n",
    "        self.values = range(35, 44)\n",
    "        self.expected_probs = np.array(pmf.Probs(self.values))\n",
    "\n",
    "    def RunModel(self):\n",
    "        np.random.shuffle(self.pool)\n",
    "        data = self.pool[:self.n], self.pool[self.n:]\n",
    "        return data\n",
    "    \n",
    "    def TestStatistic(self, data):\n",
    "        firsts, others = data\n",
    "        stat = self.ChiSquared(firsts) + self.ChiSquared(others)\n",
    "        return stat\n",
    "\n",
    "    def ChiSquared(self, lengths):\n",
    "        hist = thinkstats2.Hist(lengths)\n",
    "        observed = np.array(hist.Freqs(self.values))\n",
    "        expected = self.expected_probs * len(lengths)\n",
    "        stat = sum((observed - expected)**2 / expected)\n",
    "        return stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FalseNegRate(data, num_runs=1000):\n",
    "    \"\"\"Computes the chance of a false negative based on resampling.\n",
    "\n",
    "    data: pair of sequences\n",
    "    num_runs: how many experiments to simulate\n",
    "\n",
    "    returns: float false negative rate\n",
    "    \"\"\"\n",
    "    group1, group2 = data\n",
    "    count = 0\n",
    "\n",
    "    for i in range(num_runs):\n",
    "        sample1 = thinkstats2.Resample(group1)\n",
    "        sample2 = thinkstats2.Resample(group2)\n",
    "        ht = DiffMeansPermute((sample1, sample2))\n",
    "        p_value = ht.PValue(iters=101)\n",
    "        if p_value > 0.05:\n",
    "            count += 1\n",
    "\n",
    "    return count / num_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** As sample size increases, the power of a hypothesis test increases, which means it is more likely to be positive if the effect is real. Conversely, as sample size decreases, the test is less likely to be positive even if the effect is real.\n",
    "\n",
    "To investigate this behavior, run the tests in this chapter with different subsets of the NSFG data. You can use `thinkstats2.SampleRows` to select a random subset of the rows in a DataFrame.\n",
    "\n",
    "What happens to the p-values of these tests as sample size decreases? What is the smallest sample size that yields a positive test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import first\n",
    "\n",
    "def comparePregLength(df, iters = 1000):\n",
    "    \"\"\"\n",
    "    Gets the p-value\n",
    "    \"\"\"\n",
    "    \n",
    "    firsts = df[df.birthord == 1]\n",
    "    others = df[df.birthord != 1]\n",
    "\n",
    "    lngth_data = firsts.prglngth.values, others.prglngth.values\n",
    "    \n",
    "    lngth_ht = DiffMeansPermute(lngth_data)\n",
    "    lngth_pvalue = lngth_ht.PValue(iters = iters)\n",
    "    \n",
    "    return lngth_pvalue\n",
    "    \n",
    "    \n",
    "    \n",
    "def compareWeight(df, iters = 1000):\n",
    "    \"\"\"\n",
    "    Gets the p-value\n",
    "    \"\"\"\n",
    "    firsts = df[df.birthord == 1]\n",
    "    others = df[df.birthord != 1]\n",
    "    \n",
    "    weight_data = (firsts.totalwgt_lb.dropna().values,\n",
    "            others.totalwgt_lb.dropna().values)\n",
    "    \n",
    "    weight_ht = DiffMeansPermute(weight_data)\n",
    "    weight_pvalue = weight_ht.PValue(iters = iters)\n",
    "    \n",
    "    return weight_pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chiSquaredTest(df, iters = 1000):\n",
    "    \"\"\"\n",
    "    Gets the p-value for pregnancy length using Chi-Squared Test\n",
    "    \"\"\"\n",
    "    \n",
    "    firsts = df[df.birthord == 1]\n",
    "    others = df[df.birthord != 1]\n",
    "\n",
    "    length_data = firsts.prglngth.values, others.prglngth.values\n",
    "        \n",
    "    ht = PregLengthTest(length_data)\n",
    "    p_value = ht.PValue(iters = iters)\n",
    "    \n",
    "    return p_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlationTest(df, iters = 1000):\n",
    "    \"\"\"\n",
    "    Test correlation\n",
    "    \"\"\"\n",
    "    cleaned = df.dropna(subset=['agepreg', 'totalwgt_lb'])\n",
    "    clean_data = cleaned.agepreg.values, cleaned.totalwgt_lb.values\n",
    "    \n",
    "    ht = CorrelationPermute(clean_data)\n",
    "    pvalue = ht.PValue(iters = iters)\n",
    "    \n",
    "    return pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9148:\t0.163\t0.0\t0.0\t0.0\n",
      "4574:\t0.973\t0.001\t0.0\t0.0\n",
      "2287:\t0.158\t0.257\t0.002\t0.013\n",
      "1143:\t0.039\t0.161\t0.011\t0.262\n",
      "571:\t0.141\t0.446\t0.36\t0.112\n",
      "285:\t0.821\t0.494\t0.067\t0.266\n"
     ]
    }
   ],
   "source": [
    "n = len(live)\n",
    "\n",
    "for i in range(6):\n",
    "    sample = thinkstats2.SampleRows(live, n)\n",
    "    prgLngth = comparePregLength(sample)\n",
    "    weight = compareWeight(sample)\n",
    "    chi = chiSquaredTest(sample)\n",
    "    corr = correlationTest(sample)\n",
    "    \n",
    "    print(f\"{n}:\\t{prgLngth}\\t{weight}\\t{chi}\\t{corr}\")\n",
    "    \n",
    "    n = n // 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern is still a bit erratic, but as the sample size becomes smaller, the tests become negative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
